{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# working with Kaggle set first, then to rt-polarity to check acc, then transcript sentences\n",
    "\n",
    "train_data=pd.read_csv('/research/home/rakesh/notebooks/dennybritz/keras_example_dataset/train.csv')\n",
    "test_data=pd.read_csv('/research/home/rakesh/notebooks/dennybritz/keras_example_dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2)\n",
      "id        0\n",
      "text      0\n",
      "author    0\n",
      "dtype: int64\n",
      "id      0\n",
      "text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape,test_data.shape)\n",
    "print(train_data.isnull().sum())\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting author names to cateogories\n",
    "\n",
    "authors=train_data.author.unique()\n",
    "dic={}\n",
    "for i,author in enumerate(authors):\n",
    "    dic[author]=i\n",
    "labels=train_data.author.apply(lambda x:dic[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the train data itself into train and validate - before running on eval\n",
    "\n",
    "val_data=train_data.sample(frac=0.2,random_state=200)\n",
    "train_data=train_data.drop(val_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=train_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23090 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=20000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "sequences_valid=tokenizer.texts_to_sequences(val_data.text)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (15663, 861) (3916, 861)\n",
      "Shape of label train and validation tensor: (15663, 3) (3916, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "y_train = to_categorical(np.asarray(labels[train_data.index]))\n",
    "y_val = to_categorical(np.asarray(labels[val_data.index]))\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate own embedding - word2vec can also be tried here\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 393s 25ms/step - loss: 1.1507 - acc: 0.4113 - val_loss: 1.1243 - val_acc: 0.4242\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 448s 29ms/step - loss: 1.0937 - acc: 0.4986 - val_loss: 1.0705 - val_acc: 0.5105\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 333s 21ms/step - loss: 1.0111 - acc: 0.6167 - val_loss: 0.9822 - val_acc: 0.6964\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 321s 20ms/step - loss: 0.8811 - acc: 0.7630 - val_loss: 0.8845 - val_acc: 0.7383\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 301s 19ms/step - loss: 0.7492 - acc: 0.8221 - val_loss: 0.8019 - val_acc: 0.7837\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 311s 20ms/step - loss: 0.6488 - acc: 0.8658 - val_loss: 0.7540 - val_acc: 0.7909\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 306s 20ms/step - loss: 0.5777 - acc: 0.8981 - val_loss: 0.7322 - val_acc: 0.7972\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 315s 20ms/step - loss: 0.5261 - acc: 0.9183 - val_loss: 0.7162 - val_acc: 0.8085\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 310s 20ms/step - loss: 0.4868 - acc: 0.9367 - val_loss: 0.7069 - val_acc: 0.8131\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 310s 20ms/step - loss: 0.4566 - acc: 0.9461 - val_loss: 0.7026 - val_acc: 0.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d5370c668>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n",
    "         callbacks=callbacks)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
